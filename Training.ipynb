{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "conda install -c dglteam/label/cu118 dgl\n",
    "conda install pyg -c pyg\n",
    "conda install scikit-learn pandas pyyaml ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.function as fn\n",
    "from dgl.data import DGLDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с датасетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_dataset_VK/train.csv')\n",
    "test  = pd.read_csv('train_dataset_VK/test.csv')\n",
    "attr  = pd.read_csv('train_dataset_VK/attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego_id</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>t</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>84</td>\n",
       "      <td>148.0</td>\n",
       "      <td>5.669200e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>164</td>\n",
       "      <td>396.7</td>\n",
       "      <td>6.246274e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>594.5</td>\n",
       "      <td>4.962974e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>219</td>\n",
       "      <td>45.5</td>\n",
       "      <td>1.237935e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122280367</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>34.6</td>\n",
       "      <td>2.307750e+00</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122280368</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>53.8</td>\n",
       "      <td>3.729143e+00</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122280369</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.286984e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122280370</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.500757e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122280371</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.773366e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122280372 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ego_id    u    v      t            x1        x2   x3\n",
       "0                      0  131   84  148.0  5.669200e-07  0.000000  0.0\n",
       "1                      0  135  164  396.7  6.246274e-02  0.000000  0.0\n",
       "2                      0   47   15    NaN  0.000000e+00  0.000000  1.0\n",
       "3                      0    5    4  594.5  4.962974e-02  0.000000  0.0\n",
       "4                      0  176  219   45.5  1.237935e+00  0.000000  0.0\n",
       "...                  ...  ...  ...    ...           ...       ...  ...\n",
       "122280367  1709396984692    3    5   34.6  2.307750e+00  1.098612  0.0\n",
       "122280368  1709396984692    1    5   53.8  3.729143e+00  3.496508  1.0\n",
       "122280369  1709396984692    1    7    1.5  4.286984e+00  0.000000  0.0\n",
       "122280370  1709396984692    5   11    2.0  3.500757e+00  0.000000  0.0\n",
       "122280371  1709396984692    3    1   22.0  5.773366e-02  0.000000  0.0\n",
       "\n",
       "[122280372 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego_id</th>\n",
       "      <th>u</th>\n",
       "      <th>age</th>\n",
       "      <th>city_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>school</th>\n",
       "      <th>university</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>68</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>778293348</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "      <td>237065842</td>\n",
       "      <td>1</td>\n",
       "      <td>82803468</td>\n",
       "      <td>238500268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>60</td>\n",
       "      <td>237065842</td>\n",
       "      <td>1</td>\n",
       "      <td>196560139</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>66</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>963209731</td>\n",
       "      <td>720783270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>308862409</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930743</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>492149712</td>\n",
       "      <td>2</td>\n",
       "      <td>769209871</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930744</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930745</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930746</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>650683235</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14930747</th>\n",
       "      <td>1709396984692</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14930748 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ego_id    u  age    city_id  sex     school  university\n",
       "0                     0  227   68         -1    1  778293348          -1\n",
       "1                     0   45   38  237065842    1   82803468   238500268\n",
       "2                     0  142   60  237065842    1  196560139          -1\n",
       "3                     0  280   66         -1    2  963209731   720783270\n",
       "4                     0   41   18         -1    2  308862409          -1\n",
       "...                 ...  ...  ...        ...  ...        ...         ...\n",
       "14930743  1709396984692    2   16  492149712    2  769209871          -1\n",
       "14930744  1709396984692   12   15         -1    1         -1          -1\n",
       "14930745  1709396984692   18   23         -1    1         -1          -1\n",
       "14930746  1709396984692    4   16  650683235    1         -1          -1\n",
       "14930747  1709396984692   10   16         -1    1         -1          -1\n",
       "\n",
       "[14930748 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ego_id</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>t</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ego_id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>-0.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0.001828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218330</td>\n",
       "      <td>-0.065169</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>-0.064033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.218330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.092008</td>\n",
       "      <td>-0.019556</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>-0.022212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.001420</td>\n",
       "      <td>-0.065169</td>\n",
       "      <td>-0.092008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.190351</td>\n",
       "      <td>-0.073750</td>\n",
       "      <td>-0.025470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>-0.019556</td>\n",
       "      <td>-0.190351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678870</td>\n",
       "      <td>0.089360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-0.001061</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>-0.073750</td>\n",
       "      <td>0.678870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.064033</td>\n",
       "      <td>-0.022212</td>\n",
       "      <td>-0.025470</td>\n",
       "      <td>0.089360</td>\n",
       "      <td>0.136608</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ego_id         u         v         t        x1        x2        x3\n",
       "ego_id  1.000000  0.001828  0.001840  0.001420  0.000228 -0.001061 -0.000929\n",
       "u       0.001828  1.000000  0.218330 -0.065169  0.011669  0.001498 -0.064033\n",
       "v       0.001840  0.218330  1.000000 -0.092008 -0.019556 -0.022957 -0.022212\n",
       "t       0.001420 -0.065169 -0.092008  1.000000 -0.190351 -0.073750 -0.025470\n",
       "x1      0.000228  0.011669 -0.019556 -0.190351  1.000000  0.678870  0.089360\n",
       "x2     -0.001061  0.001498 -0.022957 -0.073750  0.678870  1.000000  0.136608\n",
       "x3     -0.000929 -0.064033 -0.022212 -0.025470  0.089360  0.136608  1.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение датасета на CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir train\n",
    "!mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61786,), (20596,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ego_id.unique().shape, test.ego_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "lookup_train = train[\"ego_id\"].value_counts()\n",
    "unique_train = train[\"ego_id\"].unique()\n",
    "\n",
    "cum_train = np.cumsum(lookup_train[unique_train])\n",
    "cum_train = np.hstack(([0], cum_train))\n",
    "\n",
    "table_train = {unique_train[i-1] : (cum_train[i-1], cum_train[i]) for i in range(1, cum_train.shape[0])}\n",
    "\n",
    "\n",
    "def split_train_csv(id):\n",
    "    low, high = table_train[id]\n",
    "    train.iloc[low : high].to_csv(f\"train/{id}.csv\")\n",
    "\n",
    "\n",
    "lookup_test = test[\"ego_id\"].value_counts()\n",
    "unique_test = test[\"ego_id\"].unique()\n",
    "\n",
    "cum_test = np.cumsum(lookup_test[unique_test])\n",
    "cum_test = np.hstack(([0], cum_test))\n",
    "\n",
    "table_test = {unique_test[i-1] : (cum_test[i-1], cum_test[i]) for i in range(1, cum_test.shape[0])}\n",
    "\n",
    "\n",
    "def split_test_csv(id):\n",
    "    low, high = table_test[id]\n",
    "    test.iloc[low : high].to_csv(f\"test/{id}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = Pool()\n",
    "\n",
    "pool.map(split_train_csv, train[\"ego_id\"].unique())\n",
    "pool.map(split_test_csv, test[\"ego_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ego_id</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>t</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>185.7</td>\n",
       "      <td>3.839089e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>131</td>\n",
       "      <td>125</td>\n",
       "      <td>161.4</td>\n",
       "      <td>4.034464e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>56</td>\n",
       "      <td>127.0</td>\n",
       "      <td>8.554643e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>594.5</td>\n",
       "      <td>2.886418e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>63</td>\n",
       "      <td>73</td>\n",
       "      <td>127.0</td>\n",
       "      <td>4.281692e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1013</td>\n",
       "      <td>8</td>\n",
       "      <td>132</td>\n",
       "      <td>17</td>\n",
       "      <td>24.1</td>\n",
       "      <td>1.826740e+00</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>1014</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>346.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1015</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "      <td>59</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>1016</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>300.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1017</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>447.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1018 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  ego_id    u    v      t            x1        x2   x3\n",
       "0              0       8   20   19  185.7  3.839089e-04  0.000000  0.0\n",
       "1              1       8  131  125  161.4  4.034464e-01  0.000000  0.0\n",
       "2              2       8   73   56  127.0  8.554643e-05  0.000000  0.0\n",
       "3              3       8    0    4  594.5  2.886418e-01  0.000000  0.0\n",
       "4              4       8   63   73  127.0  4.281692e-07  0.000000  0.0\n",
       "...          ...     ...  ...  ...    ...           ...       ...  ...\n",
       "1013        1013       8  132   17   24.1  1.826740e+00  1.791759  0.0\n",
       "1014        1014       8   29   14  346.9           NaN  0.000000  0.0\n",
       "1015        1015       8   56   59   80.0           NaN  0.000000  0.0\n",
       "1016        1016       8   14   11  300.1           NaN  0.000000  0.0\n",
       "1017        1017       8    0   33  447.6           NaN  0.000000  0.0\n",
       "\n",
       "[1018 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.read_csv('test/8.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование датасета DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr  = pd.read_csv('train_dataset_VK/attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ohe(param):\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "    encoded_data = ohe.fit_transform(param.reshape(-1, 1))\n",
    "    \n",
    "    # pad to the size 9, we can have at most 300 friends -> 300 differnt cities -> need 9 bits to represent 300 id's\n",
    "    encoded_data = np.pad(encoded_data, ((0, 0), (300 - encoded_data.shape[1], 0)), mode='constant')\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "def make_embeds(params):\n",
    "    params = params[[\"age\", \"sex\", \"city_id\", \"school\", \"university\"]].values.T\n",
    "\n",
    "    out = np.empty((params.shape[1], 902))\n",
    "\n",
    "    out[:, 0] = params[0]\n",
    "    out[:, 1] = params[1]\n",
    "    out[:, 2:302] = make_ohe(params[2])\n",
    "    out[:, 302:602] = make_ohe(params[3])\n",
    "    out[:, 602:902] = make_ohe(params[4])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VKDataset(dgl.data.DGLDataset):\n",
    "    def __init__(self, data_path : str | Path, attr : pd.DataFrame):\n",
    "        super().__init__(\"VKDataset\")\n",
    "        self.data_path = data_path\n",
    "        self.attr = attr\n",
    "\n",
    "        # sorted only for debugging\n",
    "        self.file_names = sorted(os.listdir(data_path), key = lambda x: int(x[:-4]))\n",
    "\n",
    "        self.ohe = OneHotEncoder()\n",
    "\n",
    "        # faster lookup through attrs\n",
    "        lookup_attr = attr[\"ego_id\"].value_counts()\n",
    "        unique_attr = attr[\"ego_id\"].unique()\n",
    "\n",
    "        cum_attr = np.cumsum(lookup_attr[unique_attr])\n",
    "        cum_attr = np.hstack(([0], cum_attr))\n",
    "\n",
    "        self.table_attr = {unique_attr[i-1] : (cum_attr[i-1], cum_attr[i]) for i in range(1, cum_attr.shape[0])}\n",
    "\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        id = int(self.file_names[id][:-4])\n",
    "\n",
    "        # print(id)\n",
    "\n",
    "        edges_data = pd.read_csv(f\"{self.data_path}/{id}.csv\")\n",
    "\n",
    "        low, high = self.table_attr[id]\n",
    "        nodes_data = attr.iloc[low : high]\n",
    "\n",
    "        # missing ids filling\n",
    "        table = {nodes_data.iloc[i].u : i for i in range(nodes_data.shape[0])}\n",
    "\n",
    "        # print(nodes_data.dtypes)\n",
    "\n",
    "        m = max(edges_data[\"u\"].max(), edges_data[\"v\"].max())\n",
    "        new_data = []\n",
    "        add_num = 0\n",
    "        for i in range(m+1):\n",
    "            if i not in table:\n",
    "                table[i] = nodes_data.shape[0] + add_num\n",
    "                new_data.append([id, table[i], -1, -1, -1, -1, -1])\n",
    "                add_num += 1\n",
    "        \n",
    "        # print(nodes_data.dtypes)\n",
    "\n",
    "        nodes_data = pd.concat((nodes_data, pd.DataFrame(new_data, columns = nodes_data.columns, dtype=\"int64\")), ignore_index=True)\n",
    "\n",
    "        # print(nodes_data.dtypes)\n",
    "\n",
    "        # weird bug, where df dtypes turn to objects\n",
    "        # nodes_data = nodes_data.astype({col : \"int64\" for col in nodes_data.columns})\n",
    "\n",
    "        edges_data[\"u\"] = edges_data[\"u\"].apply(lambda x: table[x])\n",
    "        edges_data[\"v\"] = edges_data[\"v\"].apply(lambda x: table[x])\n",
    "        \n",
    "        node_features_age = torch.from_numpy(nodes_data[\"age\"].to_numpy()).float()\n",
    "        # node_features_city = torch.from_numpy(nodes_data[\"city_id\"].to_numpy())\n",
    "        node_features_sex = torch.from_numpy(nodes_data[\"sex\"].to_numpy()).float()\n",
    "\n",
    "        node_features_city = torch.from_numpy(self.ohe.fit_transform(nodes_data[[\"city_id\"]]).toarray())\n",
    "        node_features_school = torch.from_numpy(self.ohe.fit_transform(nodes_data[[\"school\"]]).toarray())\n",
    "        node_features_university = torch.from_numpy(self.ohe.fit_transform(nodes_data[[\"university\"]]).toarray())\n",
    "        # print(node_features_city)\n",
    "\n",
    "        # node_features = torch.tensor([node_features_age, node_features_sex, *node_features_city[0], *node_features_school[0], *node_features_university[0]])\n",
    "        \n",
    "        # node_features = torch.tensor([node_features_age, node_features_sex, ])\n",
    "        node_features = torch.tensor(make_embeds(nodes_data))\n",
    "        # node_features = torch.from_numpy(np.vstack((node_features_age, node_features_sex)).T)\n",
    "\n",
    "        # node_features_school = torch.from_numpy(nodes_data[\"school\"].to_numpy())\n",
    "        # node_features_university = torch.from_numpy(nodes_data[\"university\"].to_numpy())\n",
    "\n",
    "\n",
    "        # node_labels = torch.from_numpy(\n",
    "        #     nodes_data[\"Club\"].astype(\"category\").cat.codes.to_numpy()\n",
    "        # )\n",
    "\n",
    "        # edge_features_t = torch.from_numpy(edges_data[\"t\"].to_numpy())\n",
    "        edge_features_x1 = torch.from_numpy(edges_data[\"x1\"].to_numpy())\n",
    "        # edge_features_x2 = torch.from_numpy(edges_data[\"x2\"].to_numpy())\n",
    "\n",
    "        edges_src = torch.from_numpy(edges_data[\"u\"].to_numpy())\n",
    "        edges_dst = torch.from_numpy(edges_data[\"v\"].to_numpy())\n",
    "\n",
    "        # graph = dgl.graph(\n",
    "        #     (edges_src, edges_dst), num_nodes=nodes_data.shape[0]\n",
    "        # )\n",
    "\n",
    "        graph = dgl.graph(\n",
    "            (np.concatenate([edges_src, edges_dst]), np.concatenate([edges_dst, edges_src])), num_nodes=nodes_data.shape[0]\n",
    "        )\n",
    "        \n",
    "        # graph.ndata[\"age\"] = node_features_age\n",
    "        # graph.ndata[\"city_id\"] = node_features_city\n",
    "        # graph.ndata[\"sex\"] = node_features_sex\n",
    "\n",
    "        # graph.ndata[\"school\"] = node_features_school\n",
    "        # graph.ndata[\"university\"] = node_features_university\n",
    "\n",
    "        graph.ndata[\"feature\"] = node_features\n",
    "\n",
    "        # graph.edata[\"t\"] = torch.concatenate((edge_features_t, edge_features_t))\n",
    "        graph.edata[\"x1\"] = torch.concatenate((edge_features_x1, edge_features_x1))\n",
    "        # graph.edata[\"x2\"] = edge_features_x2\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "\n",
    "        # n_nodes = nodes_data.shape[0]\n",
    "        # n_train = int(n_nodes * 0.6)\n",
    "        # n_val = int(n_nodes * 0.2)\n",
    "        # train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        # val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        # test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        # train_mask[:n_train] = True\n",
    "        # val_mask[n_train : n_train + n_val] = True\n",
    "        # test_mask[n_train + n_val :] = True\n",
    "        # graph.ndata[\"train_mask\"] = train_mask\n",
    "        # graph.ndata[\"val_mask\"] = val_mask\n",
    "        # graph.ndata[\"test_mask\"] = test_mask\n",
    "\n",
    "        return graph #, node_features, edge_features_x1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "    # def getitem(self, i):\n",
    "    #     return self.graph\n",
    "\n",
    "    # def len(self):\n",
    "    #     return 1\n",
    "\n",
    "# dataset = VKDataset(\"data/train\", attr)\n",
    "# graph = dataset[0]\n",
    "# # 53373\n",
    "# print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = VKDataset(\"train\", attr)\n",
    "dataloader_train = dgl.dataloading.GraphDataLoader(dataset_train, batch_size = 1024, shuffle = True, drop_last=False, num_workers=4)\n",
    "\n",
    "# dataset_test = VKDataset(\"data/test\", attr)\n",
    "# dataloader_test = dgl.dataloading.GraphDataLoader(dataset_test, batch_size = 10, drop_last=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv_in = dglnn.SAGEConv(\n",
    "            in_feats=in_feats, out_feats=hid_feats, aggregator_type='pool')\n",
    "        self.conv2 = dglnn.SAGEConv(\n",
    "            in_feats=hid_feats, out_feats=hid_feats, aggregator_type='pool')\n",
    "        self.conv_last = dglnn.SAGEConv(\n",
    "            in_feats=hid_feats, out_feats=out_feats, aggregator_type='lstm')\n",
    "\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv_in(graph, inputs)\n",
    "        h = F.tanh(h)\n",
    "        h = self.conv_last(graph, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class DotProductPredictor(nn.Module):\n",
    "    def forward(self, graph, h):\n",
    "        # h contains the node representations computed from the GNN defined\n",
    "        # in the node classification section (Section 5.1).\n",
    "        with graph.local_scope():\n",
    "            graph.ndata['h'] = h\n",
    "            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return graph.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_layer = nn.Linear(\n",
    "            in_features=in_features, out_features=hidden_features\n",
    "        )\n",
    "        self.encoder_output_layer = nn.Linear(\n",
    "            in_features=hidden_features, out_features=out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        activation = self.encoder_hidden_layer(features)\n",
    "        activation = torch.relu(activation)\n",
    "        code = self.encoder_output_layer(activation)\n",
    "        return code\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=902, hidden_features=50, out_features=5):\n",
    "        super().__init__()\n",
    "        self.sage = SAGE(128, hidden_features, out_features)\n",
    "        self.pred = DotProductPredictor()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.node_encoder = Encoder(in_features, 256, 128).float()\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        x = self.node_encoder(x)\n",
    "        h = self.sage(g, x)\n",
    "        h = self.pred(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f70c9c5ee843a7b52cdfb01164000f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ad14c6bf064287b959809342cb767d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6b8e27ccb84e1cad0e24591d75b999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddf891c26cd4f7986e736d930d480c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceb15c127164cc09bc434b3f0d31abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88301719cd2244b7aa993dc1d996d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64a887d1b07470a89e6380cce706e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b4160c71a14093b5ff54c108fb2d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f7f3f71bf54b5dabb0f412dd7e6603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26f03fb60ae4eb7bcd3032ba8d3e466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[39m# shuffle = np.random.permutation(len(dataset_train))\u001b[39;00m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n",
      "\u001b[0;32m----> 7\u001b[0m     \u001b[39mfor\u001b[39;00m graph \u001b[39min\u001b[39;00m (pbar \u001b[39m:=\u001b[39m tqdm(dataloader_train)):\n",
      "\u001b[1;32m      8\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m      9\u001b[0m             \u001b[39mif\u001b[39;00m graph\u001b[39m.\u001b[39mnum_edges() \u001b[39m>\u001b[39m \u001b[39m10_000\u001b[39m: \u001b[39m# skip very large graphs\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n",
      "\u001b[0;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n",
      "\u001b[1;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n",
      "\u001b[1;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n",
      "\u001b[1;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n",
      "\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n",
      "\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n",
      "\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n",
      "\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n",
      "\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n",
      "\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n",
      "\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n",
      "\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data()\n",
      "\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n",
      "\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n",
      "\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n",
      "\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_get_data()\n",
      "\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n",
      "\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n",
      "\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n",
      "\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n",
      "\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n",
      "\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n",
      "\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_queue\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39mtimeout)\n",
      "\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n",
      "\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n",
      "\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n",
      "\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n",
      "\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n",
      "\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n",
      "\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n",
      "\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n",
      "\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n",
      "\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n",
      "\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n",
      "\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n",
      "\u001b[0;32m--> 423\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39m], timeout)\n",
      "\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n",
      "\u001b[1;32m    927\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n",
      "\u001b[1;32m    929\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;32m--> 930\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39mselect(timeout)\n",
      "\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "\u001b[1;32m    932\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/moscow/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n",
      "\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selector\u001b[39m.\u001b[39mpoll(timeout)\n",
      "\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# shuffle = np.random.permutation(len(dataset_train))\n",
    "for epoch in range(10):\n",
    "    for graph in (pbar := tqdm(dataloader_train)):\n",
    "        try:\n",
    "            if graph.num_edges() > 10_000: # skip very large graphs\n",
    "                continue\n",
    "\n",
    "            node_features, label = graph.ndata[\"feature\"], graph.edata[\"x1\"]\n",
    "\n",
    "            graph, node_features, label = (graph.to(device), node_features.to(device).float(), label.to(device).float())\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            pred = model(graph, node_features)\n",
    "            pred = pred.squeeze()\n",
    "            loss = loss_func(label, pred)\n",
    "            # loss = ((pred - label) ** 2).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            pbar.set_description(f\"loss: {loss.item():.3f}\")\n",
    "            \n",
    "            # if i > 10000:\n",
    "            #     break\n",
    "        except Exception:\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    torch.save(model.state_dict(), f\"model/849_{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
